 \documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
%\usepackage{hyphenat}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage[binary-units=true]{siunitx}
\usepackage{ulem}
%\usepackage{censor}
\usepackage[table]{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tabularx}

\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{listings}
\usepackage{dingbat}
\usepackage{mathtools}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=blue,
    filecolor=black,
    urlcolor=blue}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\makeatletter
  \def\footnoterule{\kern-3\p@
  \hrule \@width 2in \kern 2.6\p@} % the \hrule is .4pt high
\makeatother


\begin{document}

\newcommand{\fslspm}{FSL-SPM\xspace}
\newcommand{\fslafni}{FSL-AFNI\xspace}
\newcommand{\afnispm}{AFNI-SPM\xspace}
\newcommand{\tristan}[1]{\color{orange}\textbf{From Tristan:} #1\color{black}\xspace}
\newcommand{\ali}[2]{\color{green}\textbf{Ali:} #1\color{black}\xspace}
\newcommand{\discuss}[1]{\uwave{#1}}


\title{Comparing tool variability and numerical variability in fMRI analyses}

\author{Ali Salari$^1$, Other Authors$^1$, Tristan Glatard$^1$ \\
$^1$ Department of Computer-Science and Software Engineering, Concordia University, Montreal, Canada}

\maketitle
\begin{abstract}

Numerical and software variability broadly affect structural, diffusion, and functional MRI analyses. Numerical
variability originates in software updates or code
parallelization, whereas software variability reflects discrepancies between
models implemented in different analysis software packages. While numerical
and software variability were both shown to impact analysis outcomes, the
extent to which these sources of variability compare for a given
analysis remains understudied. This work presents a comparison of
numerical and software variability for group-level and subject-level functional MRI analyses.
We reproduced a previous comparison between functional MRI analysis
software packages FSL, AFNI, and SPM, which we extended to measure
numerical variability through Monte-Carlo arithmetic.
Although We found that between-tool variability was larger than numerical variability,
we identified brain regions that numerical perturbations simulated between-tool variability.
Furthermore, we found more numerical instability in thresholded maps than unthresholded ones
and individual analysis than group-level analysis results.

\end{abstract}

\begin{IEEEkeywords}
  Numerical Instability, Reproducibility, Monte-Carlo arithmetic, Neuroimaging
\end{IEEEkeywords}


\section{Introduction}

% Data analysis workflows in many scientific domains have become increasingly complex and flexible.
Recent studies highlighted the instability of the neuroimaging pipelines depending on the computing platform,
software package, and even tool versions. Changes in the computational
environment such as compilers, libraries, operating systems may introduce small numerical errors and create
significantly different results in unstable pipelines~\cite{Glatard2015,Gronenschild2012,salari2020spot}.
Moreover, the impact of methodological changes on fMRI analyses has been investigated extensively~\cite{bowring2019exploring,botvinik2020variability,bhagwat2021understanding,carp2012plurality}.
For instance, in related works, it has been shown that running the same fMRI experiments by different teams can substantially affect
scientific conclusions~\cite{botvinik2020variability,carp2012plurality};
replication of fMRI experiments using the three most well-known software packages can influence the final determining areas of
brain activations~\cite{bowring2019exploring}; %bowring2021isolating.
also, the choice of preprocessing pipelines on neuroimaging cortical surface analyses is compound with instabilities~\cite{bhagwat2021understanding}.
% In the presence of such instabilities, it is often hard to trust the data processing results. % validity of the computational results.

In such a heterogeneous environment, numerical instability is an essential issue for reproducibility.
Numerical instability is a characteristic of pipelines that results from the influence of the floating-point arithmetics
and iterative convergence of numerical errors~\cite{freitas2002issue}.
Stochastic arithmetic approaches such as Monte-Carlo arithmetic (MCA)~\cite{Parker1997-qq} have been used to study the impact of numerical errors
originating in floating-point computations in mathematical libraries.
In~\cite{salari2021accurate}, we quantified the numerical stability of the HCP preprocessing pipeline~\cite{glasser2013} based on the MCA method
by creating a Fuzzy environment, so that instrument mathematical functions are implemented in mathematical system libraries.
As a result of numerical perturbations, we discovered a very low numerical precision in the result images comparable to the operating system variability.
In a related study~\cite{kiar2020numerical}, the instability of results was explored by instrumenting a connectome estimation pipeline with the MCA technique.
These works demonstrate the necessity of numerical uncertainty quantification for understanding related issues that hamper the computational reproducibility of analyses.

In this work, we reproduce an fMRI analysis in~\cite{bowring2019exploring} with different neuroimaging software packages in the presence of the Fuzzy environment,
and then quantify the variability across tools and the numerical variability in results.
We call the cross-software variation and numerical variation, between tool (BT) and within tool (WT) variability, respectively.
In fact, the WT variability shows results across software packages in the Fuzzy environment.
The primary objective of this study is to answer these two questions: 1) how the fMRI analyses across tools are numerically stable?
2) how the numerical variability is in comparison with the tool variability?
This comparative study reveals the importance of numerical variability and motivates research studies to evaluate the numerical uncertainty of the pipelines.

% We start to reproduce an analysis from Bowring, this can be as a practice for reproducibility manner in the community.
% In particular, we investigate the effect of 1) between software 2) within software (numerical)
% We present a comparative assessment of group-level analysis of an fMRI pipeline.


\section{Materials and Methods}

\subsection{fMRI analysis \& Dataset}

We replicated the fMRI analysis described as study `ds000001'
in~\cite{schonberg2012decreasing}, relying on the data publicly available
in OpenNeuro at \url{https://openneuro.org/datasets/ds000001} and using the
three main software packages for fMRI data processing, namely FMRIB
Software Library (FSL)~\cite{jenkinson2012fsl}, Analysis of Functional
NeuroImages (AFNI)~\cite{cox1996afni}, and Statistical Parametric
Mapping (SPM)~\cite{penny2011statistical}. We selected this dataset because
comparable analysis pipelines implemented in FSL, AFNI and SPM were already 
publicly available and extensively described in~\cite{bowring2019exploring}.
Furthermore, the work in~\cite{bowring2019exploring} already evaluated the
effect of tool variability for this dataset, which we intended to
complement with the present quantification of numerical variability.

In the selected study, 16 healthy adult subjects participated in the
balloon analog risk task~\cite{lejuez2002evaluation} to measure risk-taking
behavior over three scanning sessions~\cite{schonberg2012decreasing}. We
reused the preprocessing, first-level, and second-level analyses
implemented by~\cite{bowring2019exploring} consistently across all three
software packages. Table~\ref{table:pipeline-steps} summarizes the analytical steps in each
pipeline. More details are available in~\cite{bowring2019exploring}.


%%%%%%%%%% Summary of statstics %%%%%%%%
\setlength{\tabcolsep}{4pt}
\begin{table}[h]
    \centering
    \begin{tabular}{|c|l|c|c|c|}
        \hline
%        \multirow{2}{*}{} & \multicolumn{1}{c}{Thresholded}& & \multicolumn{1}{c}{Unthresholded}& \\
        \multicolumn{2}{|c|}{} & FSL & AFNI & SPM \\
        \hline
        {Preprocessing} & {Motion Correction}                          & \checkmark    & \checkmark     & \checkmark  \\
        {} & {Segmentation}                               &    &      & \checkmark  \\
        {} & {Brain Extraction (Anatomical)}              & \checkmark     & \checkmark    & \checkmark  \\
        {} & {Brain Extraction (Functional)}              &   & \checkmark     &  \\
        {} & {Intra-subject Coregistration}               & \checkmark    & \checkmark     & \checkmark \\
        {} & {Inter-subject Registration}                 & \checkmark    & \checkmark     & \checkmark \\
        {} & {Analysis Voxel Size}                        & \checkmark    & \checkmark     & \checkmark \\
        {} & {Smoothing}                                  & \checkmark    & \checkmark     & \checkmark  \\
        \hline
        {First-level} & {Model Specification}                          & \checkmark    & \checkmark     & \checkmark  \\
        {} & {Inclusion of 6 Motion Parameters}                               & \checkmark   &  \checkmark    & \checkmark  \\
        {} & {Model Estimation}                           & &     & \checkmark  \\
        {} & {Contrasts}                                   &  \checkmark & \checkmark     & \checkmark \\
        \hline
        {Second-level} & {Model Specification}                          & \checkmark    & \checkmark     & \checkmark  \\
        {} & {Model Estimation}                           &      &    & \checkmark  \\
        {} & {Contrasts}                                   &   & \checkmark     & \checkmark  \\
        {} & {Second-level Inference}                               &  \checkmark  &    \checkmark  & \checkmark  \\
        \hline

      \end{tabular}
    \caption{Software processing steps (adapted from~\cite{bowring2019exploring}).}
    \label{table:pipeline-steps}
\end{table}

\subsection{Fuzzy Libmath environment}

To introduce controlled amounts of numerical noise in the analyses, we used
the Fuzzy Libmath library~\cite{salari2021accurate}, a version of the GNU
mathematical library (libmath) instrumented with Monte-Carlo arithmetic.
Monte-Carlo arithmetic simulates roundoff and catastrophic cancellation
errors by introducing a controlled amount of noise in the floating-point
operations through the following perturbation model~\cite{Parker1997-qq}:

\begin{equation} \label{eq:mca_inexact}
  inexact(x) = x + 2^{e_x-t}\xi
\end{equation}

where $e_x$ is the exponent in the floating-point representation of $x$,
$t$ is the virtual precision --- the number of unperturbed bits in the
mantissa of $x$ ---, and $\xi$ is a random uniform variable of
$(-\frac{1}{2}, \frac{1}{2})$. The perturbation is introduced using 
Verificarlo~\cite{denis2015verificarlo}, an LLVM compiler supporting Monte-Carlo 
arithmetic and other types of numerical analyses.

The instrumented libmath functions are loaded in the pipeline using
LD\_PRELOAD, a Linux mechanism to force load a shared library into an
executable. This allows functions defined in Fuzzy Libmath to transparently
overload the original ones without the need to modify or recompile the
analysis pipeline.

Fuzzy Libmath only introduces perturbations in the output values of
mathematical functions but not in their input values or within their
implementation. This is done by wrapping the original functions and
applying function \texttt{inexact} to their returned values.
Listing~\ref{algo:wrapper} shows an example of this wrapping for the
\texttt{log} function in single and double precision. In this wrapper, the
original function is called through \texttt{dlsym}, a function that returns
the memory address of a symbol --- in our case \texttt{RTLD\_NEXT}, the
address of the next occurrence of the function in memory. Function wrappers
are compiled with Verificarlo. Verificarlo instruments the result of the
addition between the original function output and a floating-point zero.

\lstdefinestyle{customasm}{
  belowcaptionskip=1\baselineskip,
  frame=L,
  xleftmargin=\parindent,
  language=[x86masm]Assembler,
  basicstyle=\footnotesize\ttfamily,
  commentstyle=\itshape\color{purple!40!black},
}
\lstinputlisting[caption=Sample wrapper function (C code), label=algo:wrapper, style=customasm]{wrapper.c}

\tristan{
Explain how the instrumentation was validated:
\begin{enumerate}
\item Verify that two consecutive IEEE executions are identical (no random effects)
\item Verify that all pipelines use libm
\item List all the dependencies of all the pipelines using ldd 
\item Verify that no math function is implemented in these dependencies
\item Verify that fuzzy libmat has an effect on the results
\end{enumerate}
}

\subsection{Data processing}

We measured between-tool (BT) variability by running the pipelines
described in~\cite{bowring2019exploring} with FSL version 5.0.10, AFNI
version 18.1.09, and SPM12 version r7771 executed with GNU/Octave version
5.2. All these software versions were identical to the study
in~\cite{bowring2019exploring} except for SPM for which we used Octave
instead of MATLAB to enable mathematical function instrumentation using
Fuzzy Libmath. Indeed, MATLAB uses its own built-in mathematical functions,
which prevents the use of Fuzzy Libmath. In AFNI, we set the number of
threads to 7 \discuss{to reproduce the results
in~\cite{bowring2019exploring} in a feasible execution time}. All the
analyses were conducted on the CentOS 7.3 operating system. The
computations were performed on \href{https://www.computecanada.ca}{Compute
Canada's} Béluga cluster nodes, each with 2$\times$ Intel Gold 6148 Skylake
@ 2.4 GHz (40 cores/node) CPU and 8~GB of RAM per core. We encapsulated the
above-mentioned software packages in Docker container images available at
\url{https://github.com/ali4006/fuzzy-neurotools/tree/main/dockerfile}
\tristan{Ali, could you put them in the paper repo unless there is a good
reason to keep them separate?}.

We measured within-tool (WT) variability by running the same analyses three
times using Fuzzy Libmath with a virtual precision of $t=53$~bits for
double-precision values and $t=24$~bits for single-precision values. These
values were chosen such that the numerical perturbation simulates machine
error, the relative error between a real number and its floating-point
representation. The resulting samples are equally plausible estimates of
the true numerical result. Moreover, we repeated the FSL analyses for virtual
precisions ranging from $t=1$~bit to $t=24$~bits for single-precision
and double-precision values. We also repeated the FSL analyses for virtual 
precisions ranging from $t=24$~bits to $t=53$~bits for double-precision values, having 
set the virtual precision to $t=24$~bits for single-precision values. 

We evaluated WT and BT variability for thresholded as well as unthresholded
group-level and subject-level t-statistics maps, by computing the
standard deviation of t-statistic maps. For BT, we computed the standard deviation 
across a given pair of tools.
For WT, we computed the standard deviation across 
Fuzzy Libmath samples of a given tool. Moreover, we computed WT for a pair of tools (A, B) as follows:
\begin{equation}
  \sigma_{WT(A,B)}^2 = \sigma_{WT(A)}^2 + \sigma_{WT(B)}^2,
  \label{eq:wt-pair}
\end{equation}
where $WT(.)$ is the within-tool variability for a given tool.

Further, from the thresholded maps, we determined regional Dice
coefficients between activation clusters in the 360 regions in the Human
Connectome Project Multi-Modal Parcellation atlas version 1.0
(HCP-MMP1.0)~\cite{glasser2016multi}. For BT, Dice coefficients were
computed between activation maps produced by any pair of tools. For WT,
Dice coefficients were computed pair-wise among the three Fuzzy samples of
a given tool and averaged. All Dice coefficients were normalized by the
region size \tristan{explain how you did this: x = (dice x region size) ; x = (x-xmin)/(xmax-xmin)}. 

\section{Results}
All scripts and data to reproduce the results are
available at \url{https://github.com/big-data-lab-team/fuzzy-neurotools}.


\subsection{Sanity check}

We verified the correctness of our analyses by comparing our unperturbed
t-statistic group maps with the ones obtained
in~\cite{bowring2019exploring}. For FSL, we found
identical checksums. For SPM, the checksums were different but differences
were visually imperceptible. For AFNI, the activation maps were similar
overall, however, differences were more noticeable visually. 
Overall, our results correctly reproduced
the ones presented in~\cite{bowring2019exploring}. The observed
differences were small and might be due to the use of Octave vs MATLAB in
SPM, different operating systems \tristan{had you checked that?}, or
different hardware. We performed visual quality control of the AFNI and SPM results
for each individual subject.
\tristan{add a figure to show the differences for SPM (Octave) and AFNI}

\subsection{Summary group-level statistics}

Summary statistics for the group-level unthresholded and thresholded
t-statistics are reported in Table~\ref{table:pipeline-stats}.
\tristan{From here this paragraph needs a full re-write} Overall, BT
variability was larger than WT variability for both means and standard
deviations \tristan{I don't think it's the case for stdev}. This was
confirmed by computing the T-test and Wilcoxon signed-rank statistics
between $BT(A, B)$, and $WT(A)$ and $WT(B)$ for $A$ and $B$ in $[FSL, SPM,
AFNI]$. We obtained statistically significant values between BT and WT
variability with t-value $> 70$ and p-value $< 0.05$ in both thresholded
and unthresholded maps.

%%%%%%%%%% Summary of statstics %%%%%%%%
\setlength{\tabcolsep}{5pt}
\begin{table}[h]
    \centering
    \begin{tabular}{cccc|cc}
        \toprule
        \multirow{2}{*}{}& {} & \multicolumn{2}{c}{Thresholded} & \multicolumn{2}{c}{Unthresholded} \\
        \cmidrule{3-4} \cmidrule{5-6} \\
        {} & {} & Mean & Std. dev. & Mean & Std. dev. \\
        \midrule
        \rowcolor{lightgray}
        {Between Tools} & FSL vs. SPM        &  1.282       & 0.525      & 0.443     & 0.344  \\
        \rowcolor{lightgray}
        {(BT)} & FSL vs. AFNI                &  1.548       & 0.616      & 0.547     & 0.441  \\
        \rowcolor{lightgray}
        {} & AFNI vs. SPM                    &  1.475       & 0.672      & 0.608     & 0.477  \\
        {Within Tool} & FSL                  &  0.354       & 0.491      & 0.082     & 0.065  \\
        {(WT)}   & SPM                       &  0.252       & 0.448      & 0.054     & 0.045  \\
        {}   & AFNI                          &  0.434       & 0.524      & 0.128     & 0.135  \\
        \bottomrule
    \end{tabular}
    \caption{Summary of voxel-wise mean and standard deviation in
    group-level t-statistics in BT and WT \tristan{The caption doesn't
    reflect what I thought this table represented, this needs
    discussion. Also in Table III.}.}
    \label{table:pipeline-stats}
\end{table}


\subsection{Group-level unthresholded maps}
% \subsection{Variability of unthresholded statistical maps}

Figure~\ref{fig:unthresh-maps} compares t-statistics standard deviation maps
across different pipelines (BT variability) and across Fuzzy Libmath
samples (WT variability). While WT variability remains lower than BT
variability overall, it approaches it in some regions, shown in white in
Figure~\ref{fig:unthresh-maps}-\textbf{C}. 

Interestingly, a group of voxels \tristan{...}
BT and WT variability appear correlated in some regions
(Figure~\ref{fig:unthresh-correlation}).

%%%%%%%%%% Var. of Unthresh %%%%%%%%
\begin{figure*}[ht]
    \fbox{\begin{minipage}{\dimexpr \textwidth-2\fboxsep-2\fboxrule}
      \centering
      \includegraphics[width=\textwidth]{figures/std/gl-unthresh.png}
      %\caption{Standard deviation of thresholded t-statistics map on template surface}
    \caption{Unthresholded group t-statistics standard deviations computed between tools (\textbf{A}) and within tools (\textbf{B}), and difference between them (\textbf{C}).}
    \label{fig:unthresh-maps}
    \end{minipage}}
  \end{figure*}

  %%%%%%%%%% Corr. plot of tstats%%%%%%%%
  \begin{figure*}[ht]
    \fbox{\begin{minipage}{\dimexpr \textwidth-2\fboxsep-2\fboxrule}
      \begin{subfigure}[ht]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/std-corr-unthresh-plot.png}
        %\caption{Standard deviation of thresholded t-statistics map on template surface}
      \end{subfigure}
      \hfill
      \begin{subfigure}[ht]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/std/correlated-unthresh.png}
        %\caption{Standard deviation of thresholded t-statistics map on template surface}
      \end{subfigure}
      \caption{
        Voxel-wise comparison of BT and WT variabilty. The bottom row represents 
        voxels where the BT/WT ratio is in (0.5, 2).}
    \label{fig:unthresh-correlation}
    \end{minipage}}
  \end{figure*}

  \subsection{Subject-level unthresholded maps}

  Summary statistics for subject-level unthresholded t-statistics maps are
  reported in Table~\ref{table:unthresh-maps-subjects}. Overall, results
  represent similar findings with group-level t-statistics between WT and
  BT variations.

  Figure~\ref{fig:unthresh-maps-sbj} shows BT and WT variability of the
  unthresholded t-statistics for the subject with the highest WT
  variability among the 16 subjects.
  For this particular subject, we observed substantial WT variations with the average standard deviation 0.099 compared to 
  the average standard deviation 0.466 for BT variations.
  As can be seen in the Figure, WT variability approaches and even surpasses BT variability in some regions.
  This subject also revealed that WT variability mainly concentrated in some regions  
  contrary to the corresponding variability from unthresholded group-level t-statistics which was uniformly distributed across the brain.

  %%%%%%%%%% Summary of statstics %%%%%%%%
\setlength{\tabcolsep}{5pt}
\begin{table}[h]
    \centering
    \begin{tabular}{cccc}
        \toprule
        \multirow{2}{*}{}& {} & \multicolumn{2}{c}{Unthresholded} \\
        \cmidrule{3-4} \\
        {} & {} & Mean & Std. dev. \\
        \midrule
        \rowcolor{lightgray}
        {Between Tools} & FSL vs. SPM        &  0.366       & 0.293     \\
        \rowcolor{lightgray}
        {(BT)} & FSL vs. AFNI                &  0.439       & 0.352     \\
        \rowcolor{lightgray}
        {} & AFNI vs. SPM                    &  0.491       & 0.381     \\
        {Within Tool} & FSL                  &  0.077       & 0.054     \\
        {(WT)}   & SPM                       &  0.048       & 0.037     \\
        {}   & AFNI                          &  0.108       & 0.131     \\
        \bottomrule
    \end{tabular}
    \caption{Summary of voxel-wise mean and standard deviation in subject-level t-statistics in BT and WT. Values were averaged across 16 subjects.}
    \label{table:unthresh-maps-subjects}
\end{table}


  %%%%%%%%%% Var. of Unthresh sbj05%%%%%%%%
\begin{figure*}[ht]
  \fbox{\begin{minipage}{\dimexpr \textwidth-2\fboxsep-2\fboxrule}
    \centering
    \includegraphics[width=\textwidth]{figures/std/sbj05-std.png}
    %\caption{Standard deviation of thresholded t-statistics map on template surface}
    \caption{Unthresholded subject t-statistics standard deviations computed between tools (\textbf{A}) and within tools (\textbf{B}), and difference between them (\textbf{C}).
    This is for the subject with the highest WT variability.}
  \label{fig:unthresh-maps-sbj}
  \end{minipage}}
\end{figure*}

\subsection{WT variability across virtual precisions}

While the previous results were obtained at the virtual precision of 53~bits for double-precision floating-point values
and 24~bits for single-precision values, we evaluated WT variability across different virtual precisions for FSL.
Figure~\ref{fig:across-precisions} represents the root-mean-square error (RMSE) between the standard deviations per voxel
in BT and WT resulted from the Fuzzy FSL,
substantiating that how variabilities in both conditions are close to each other across different virtual precisions.

We determined the virtual precision of t=17 bits as the precision that minimized the RMSE between BT and WT on average.
This is the precision that numerical perturbations in FSL more closely simulated BT variability.
The brain maps of standard deviations between BT and WT obtained from the Fuzzy FSL at t=17 bits (Figure~\ref{fig:gnp-mni})
show significant WT variability, particularly for some regions in the frontal, parietal, and limbic lobes, close to BT variability.


  %%%%%%%%%% plot different precisions%%%%%%%%
  \begin{figure}[ht]
    \fbox{\begin{minipage}{\dimexpr \columnwidth-2\fboxsep-2\fboxrule}
        \centering
        \includegraphics[width=\columnwidth]{figures/rmse-precisions.png}
        %\caption{Standard deviation of thresholded t-statistics map on template surface}
      \caption{Voxel-wise RMSE between BT and WT variability maps for different virtual precisions. WT is computed from three samples of FSL tool.}
    \label{fig:across-precisions}
    \end{minipage}}
  \end{figure}

  
  %%%%%%%%%% plot different precisions%%%%%%%%
  \begin{figure}[ht]
    \fbox{\begin{minipage}{\dimexpr \textwidth-2\fboxsep-2\fboxrule}
        \centering
        \includegraphics[width=\textwidth]{figures/bg_global_precision.pdf}
        %\caption{Standard deviation of thresholded t-statistics map on template surface}
        \caption{Unthresholded group t-statistics standard deviations computed between tools (\textbf{A}) and within tools (\textbf{B}), and difference between them (\textbf{C}).
        WT is computed from three samples of FSL tool at the virtual precision of t=17 bits.}
    \label{fig:gnp-mni}
    \end{minipage}}
  \end{figure}

  \subsection{Group-level thresholded maps}

  Similar conclusions are drawn from the unthresholded t-statistics maps
  (Figure~\ref{fig:thresh-maps}).  Figure~\ref{fig:dice-thresh} compares regional normalized Dice coefficients of
  activated voxels, showing a linear correlation between BT and WT with
  p-value $< 0.05$ in all three pairs.
  
  %%%%%%%%%% Var. of Thresh %%%%%%%%
  \begin{figure*}[ht]
      \fbox{\begin{minipage}{\dimexpr \textwidth-2\fboxsep-2\fboxrule}
        \centering
        \includegraphics[width=\textwidth]{figures/std/gl-thresh.png}
      \caption{Thresholded group t-statistics standard deviation maps
      computed across tools (\textbf{A}) and Fuzzy Libmath samples
      (\textbf{B}) as defined in Eq~\ref{eq:wt-pair}. \tristan{Change the column headers to (FSL, SPM) instead of FSL - SPM (everywhere).}}
      \label{fig:thresh-maps}
      \end{minipage}}
    \end{figure*}
  
    % %%%%%%%%%% Dice plot of thresholded tstats%%%%%%%%
    \begin{figure*}[ht]
      \centering
      \includegraphics[width=\textwidth]{figures/dices_corr.png}
      \caption{Normalized regional Dice coefficients of activated voxels for
      BT and WT using the HCP-MMP1.0 parcellation of
      ~\cite{glasser2016multi}.}
      \label{fig:dice-thresh}
    \end{figure*}

\section{Discussion}

We represented differences in the results of an fMRI analysis between numerical variability
and the variability arisen from software package variations.
Although BT variability was larger than WT variability on average,
we demonstrated regions in the brain maps that numerical perturbations could simulate BT variability.
The correlation of these variabilities suggests that numerical perturbations can be used to evaluate the robustness of the analyses
to inter-software disparities.

While switching between \fslspm was generated the least BT and WT variabilities consistently, the most level of these variabilities was identified
between \fslafni and \afnispm. %We also found AFNI as the most sensitive tool to numerical perturbations.
These results do not indicate which software package is better or worse since there is no ground-truth model for the analyses.
However, this study revealed the importance of numerical instability in the selected tools for doing fMRI analyses.
It provided a view of related numerical variability of which should be taken into consideration by the software developers.

We found that numerical instability in individual analyses was attenuated in group analyses
since we obtained substantially higher variability in t-statistics at the level of subjects.
This is consistent with the findings in~\cite{bowring2021isolating} in which identified the first-level signal model as the largest
source of variability across the three tools for this study.
This is an issue for the development of fMRI biomarkers.
As a solution, we can aggregate the results obtained from numerical perturbations to improve biomarkers,
as suggested in~\cite{kiar2020numerical} where data augmentation using numerical perturbations resulted in a better classification in connectomics.

It is notable that we obtained more uncertainty on thresholded results than unthresholded maps, %This issue is 
probably due to different thresholding values used in different tools.
This is consistent with the observations in~\cite{bowring2021isolating} where identified a high correlation between unthresholded t-statistic maps,
but substantially low Dice coefficient for activations in the thresholded maps.
This raises further investigations on thresholding techniques to reduce the associated instabilities.

Given that Fuzzy Libmath only perturbs basic mathematical functions, we expect more numerical variability by perturbing linear algebra,
in particular for non-scalar types like vectors or matrices. For the future study, this can be evaluated using MCA-instrumented versions
of BLAS and LAPACK along with other libraries available in the Fuzzy project in Verificarlo's GitHub repository at github.com/verificarlo/fuzzy.


\section{Conclusion}

We presented a comparison study between numerical variability introduced by floating-point perturbations and tool variability arised from software package variations.
For an fMRI analysis, we measured a significant variability in both conditions, which was comparable for some brain regions.
We showed the Fuzzy Libmath library as a valuable model based on MCA method for simulating the numerical variability.
Moreover, we can leverage the MCA-instrumented libraries for simulating the numerical uncertainty in different studies.

Further work can be done to evaluate and localize the numerical stabilities within tools by focusing on the particular parts of
the pipeline that has been identified as the main sources of variations in~\cite{bowring2021isolating}.
This helps to identify the accurate source of instabilities within pipelines and gradually improve the stability of the pipeline components.

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{plain}
\bibliography{biblio}


\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
