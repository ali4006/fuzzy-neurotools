 \documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
%\usepackage{hyphenat}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage[binary-units=true]{siunitx}
\usepackage{ulem}
%\usepackage{censor}
\usepackage[table]{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tabularx}

\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{listings}
\usepackage{dingbat}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=blue,
    filecolor=black,
    urlcolor=blue}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\begin{document}

\newcommand{\fslspm}{FSL-SPM\xspace}
\newcommand{\fslafni}{FSL-AFNI\xspace}
\newcommand{\afnispm}{AFNI-SPM\xspace}
\newcommand{\tristan}[1]{\color{orange}\textbf{From Tristan:} #1\color{black}\xspace}
\newcommand{\note}[2]{\color{green}\textbf{Note:} #1\color{black}\xspace}



\title{Comparing tool variability and numerical variability in fMRI analyses}

\author{Ali Salari$^1$, Other Authors$^1$, Tristan Glatard$^1$ \\ 
$^1$ Department of Computer-Science and Software Engineering, Concordia University, Montreal, Canada}

\maketitle
\begin{abstract}

Numerical and software variability broadly affect structural, diffusion, and functional MRI analyses. Numerical
variability originates in software updates or code
parallelization, whereas software variability reflects discrepancies between
models implemented in different analysis software packages. While numerical
and software variability were both shown to impact analysis outcomes, the
extent to which these sources of variability compare for a given
analysis remains understudied. This work presents a comparison of
numerical and software variability for group-level functional MRI analysis
\tristan{did we give up on individual analyses?}.
We reproduced a previous comparison between functional MRI analysis 
software packages FSL, AFNI, and SPM, which we extended to measure 
numerical variability through Monte-Carlo arithmetic. 
We find that between-tool variability is an order of magnitude higher than numerical variability.
\tristan{summarize conclusions}

\end{abstract}

\begin{IEEEkeywords}
  Numerical Instability, Reproducibility, Monte-Carlo Arithmetic, Neuroimaging
\end{IEEEkeywords}


\section{Introduction}

% Data analysis workflows in many scientific domains have become increasingly complex and flexible. 
Recent studies highlighted the instability of the neuroimaging pipelines depending on the computing platform,
software package, and even tool versions. Changes in the computational
environment such as compilers, libraries, operating systems may introduce small numerical errors and create
significantly different results in unstable pipelines~\cite{Glatard2015,Gronenschild2012,salari2020spot}.
Moreover, the impact of methodological changes on fMRI analyses has been investigated extensively~\cite{bowring2019exploring,botvinik2020variability,bhagwat2021understanding,carp2012plurality}.
For instance, in related works, it has been shown that running the same fMRI experiments by different teams can substantially affect
scientific conclusions~\cite{botvinik2020variability,carp2012plurality};
replication of fMRI experiments using the three most well-known software packages can influence the final determining areas of
brain activations~\cite{bowring2019exploring}; %bowring2021isolating.
also, the choice of preprocessing pipelines on neuroimaging cortical surface analyses is compound with instabilities~\cite{bhagwat2021understanding}.
% In the presence of such instabilities, it is often hard to trust the data processing results. % validity of the computational results.

In such a heterogeneous environment, numerical instability is an essential issue for reproducibility.
Numerical instability is a characteristic of pipelines that results from the influence of the floating-point arithmetics
and iterative convergence of numerical errors~\cite{freitas2002issue}.
Stochastic arithmetic approaches such as Monte-Carlo Arithmetic (MCA)~\cite{Parker1997-qq} have been used to study the impact of numerical errors
originating in floating-point computations in mathematical libraries.
In~\cite{salari2021accurate}, we quantified the numerical stability of the HCP preprocessing pipeline~\cite{glasser2013} based on the MCA method
by creating a Fuzzy environment, so that instrument mathematical functions are implemented in mathematical system libraries.
As a result of numerical perturbations, we discovered a very low numerical precision in the result images comparable to the operating system variability.
In a related study~\cite{kiar2020numerical}, the instability of results was explored by instrumenting a connectome estimation pipeline with the MCA technique.
These works demonstrate the necessity of numerical uncertainty quantification for understanding related issues that hamper the computational reproducibility of analyses.

In this work, we reproduce an fMRI experiment with different neuroimaging software packages in the presence of the Fuzzy environment,
and then quantify the numerical variability and between tool variability in the results. 
The primary objective of this study is to answer these two questions: 1) how the fMRI analyses across tools are numerically stable?
2) how the numerical variability is in comparison with the tool variability?
This comparative study reveals the importance of numerical variability and motivates research studies to evaluate the numerical uncertainty of the pipelines.
  
% We start to reproduce an analysis from Bowring, this can be as a practice for reproducibility manner in the community.
% In particular, we investigate the effect of 1) between software 2) within software (numerical)
% We present a comparative assessment of group-level analysis of an fMRI pipeline.   


\section{Materials and Methods}

\subsection{Fuzzy libmath environment}

To simulate the machine-level uncertainty \tristan{introduce machine-level
uncertainty}, we used the method introduced in~\cite{salari2021accurate}
called Fuzzy libmath (FL). FL uses Monte Carlo Arithmetic (MCA) to
introduce a controlled amount of noise in the floating-point operations \tristan{not in all of them}
through the following perturbation model:
\begin{equation} \label{eq:mca_inexact}
  inexact(x) = x + 2^{e_x-t}\xi
\end{equation}
where $e_x$ is the exponent in the floating-point representation of $x$,
$t$ is the virtual precision, the number of bits in mantissa that will not be perturbed,
and $\xi$ is a random uniform variable of $(-\frac{1}{2}, \frac{1}{2})$.
The perturbation is applied in a given number of least-significant bits,
which models floating-point rounding and catastrophic cancellation errors.
This is automated using Verificarlo tool~\cite{denis2015verificarlo} that implements MCA at compilation time.

With the goal of OS-level perturbations \tristan{what does it mean?}, this framework instruments mathematical functions embedded
in the mathematical library \tristan{tautology} in GNU operating systems, libmath.
To avoid some pitfalls in deterministic arithmetics like producing results out of the function definition,
FL instruments the functions by wrapping them so that only the original functions' output
is perturbed instead of input or their implementation.
This is achieved by adding a floating-point zero to the output of the original function \tristan{no way to understand from the text why adding a zero would introduce noise} so that the result of this summation
is perturbed and returned.
Listing~\ref{algo:wrapper} shows an example of this wrapping for the log function in both single and double precisions.
In this script, the original functions are called by \texttt{dlsym} \tristan{this can't be understood unless the ldpreload trick is mentioned before}, a function that returns the memory address of a symbol
using the handle of \texttt{RTLD\_NEXT}.
This allows one to provide a wrapper around a function in another shared library. 

% \lstdefinestyle{customc}{
%   belowcaptionskip=1\baselineskip,
%   breaklines=true,
%   frame=L,
%   xleftmargin=\parindent,
%   language=C,
%   showstringspaces=false,
%   basicstyle=\footnotesize\ttfamily,
%   keywordstyle=\bfseries\color{green!40!black},
%   commentstyle=\itshape\color{purple!40!black},
%   identifierstyle=\color{blue},
%   stringstyle=\color{orange},
% }

\lstdefinestyle{customasm}{
  belowcaptionskip=1\baselineskip,
  frame=L,
  xleftmargin=\parindent,
  language=[x86masm]Assembler,
  basicstyle=\footnotesize\ttfamily,
  commentstyle=\itshape\color{purple!40!black},
}
\lstinputlisting[caption=Sample wrapper script, label=algo:wrapper, style=customasm]{wrapper.c}
%\lstinputlisting[caption=Scheduler, style=customc]{../wrapper2.c}


Finally, the instrumented functions are loaded in the pipeline using LD\_PRELOAD, a Linux environment variable
to force load a shared library into an executable.
This allows functions defined in FL to transparently
overload the original ones without the need to modify or recompile the analysis pipeline.

FL allows measuring the effect of numerical variability by running a program multiple times
with the feature of controlling the magnitude of the perturbations as perceived by the application.
FL enables us to assess the software packages that are dynamically linked to the mathematical libraries.
So, it is important to trace the tool dependencies of the pipelines to ensure that the specific mathematical library
is involved during the pipeline executions. \tristan{``is involved'' is required but not sufficient. How do you guarantee that 
no other math functions are called on the side?}


\subsection{fMRI analysis \& Dataset}

We replicated the fMRI analysis described as study `ds000001' in~\cite{bowring2019exploring}, relying on
the data publicly available in OpenNeuro at \url{https://openneuro.org/datasets/ds000001}
and using the three main software packages for fMRI data processing, namely
FSL~\cite{jenkinson2012fsl}, AFNI~\cite{cox1996afni}, and SPM~\cite{penny2011statistical} \tristan{expand acronyms}.
We selected this analysis because the methods and sources \tristan{what do you mean? data?} were explicitly
written to be accessible and reproducible. This enabled us to reanalyze tests \tristan{``tests'' isn't understandable} using the FL framework and
compare results with the between tool variability \tristan{``between tool variability isn't understandable"}.

In the selected study, 16 healthy adult subjects participated in the balloon analog risk task to measure
risk-taking behavior over three scanning sessions.
This study included preprocessing, first-level, and second-level analyses that were implemented with all three software.
Table~\ref{table:pipeline-steps} shows the steps that were taken in the procedure of analyses for each tool in the original study.
In the original study, a number of preprocessing steps widely accepted within the community, such as motion correction,
segmentation, brain extraction, and registration, were applied in all analyses to ensure that results from each software
package could be compared objectively.
A full description of the pipelines implemented within three packages is presented in~\cite{bowring2019exploring,schonberg2012decreasing}.


%%%%%%%%%% Summary of statstics %%%%%%%%
\setlength{\tabcolsep}{4pt}
\begin{table}[h]
    \centering
    \begin{tabular}{|c|l|c|c|c|}
        \hline
%        \multirow{2}{*}{} & \multicolumn{1}{c}{Thresholded}& & \multicolumn{1}{c}{Unthresholded}& \\
        \multicolumn{2}{|c|}{} & FSL & AFNI & SPM \\
        \hline
        {Preprocessing} & {Motion Correction}                          & \checkmark    & \checkmark     & \checkmark  \\
        {} & {Segmentation}                               &    &      & \checkmark  \\
        {} & {Brain Extraction (Anatomical)}              & \checkmark     & \checkmark    & \checkmark  \\
        {} & {Brain Extraction (Functional)}              &   & \checkmark     &  \\
        {} & {Intra-subject Coregistration}               & \checkmark    & \checkmark     & \checkmark \\
        {} & {Inter-subject Registration}                 & \checkmark    & \checkmark     & \checkmark \\
        {} & {Analysis Voxel Size}                        & \checkmark    & \checkmark     & \checkmark \\
        {} & {Smoothing}                                  & \checkmark    & \checkmark     & \checkmark  \\
        \hline
        {First-level} & {Model Specification}                          & \checkmark    & \checkmark     & \checkmark  \\
        {} & {Inclusion of 6 Motion Parameters}                               & \checkmark   &  \checkmark    & \checkmark  \\
        {} & {Model Estimation}                           & &     & \checkmark  \\
        {} & {Contrasts}                                   &  \checkmark & \checkmark     & \checkmark \\
        \hline
        {Second-level} & {Model Specification}                          & \checkmark    & \checkmark     & \checkmark  \\
        {} & {Model Estimation}                           &      &    & \checkmark  \\
        {} & {Contrasts}                                   &   & \checkmark     & \checkmark  \\
        {} & {Second-level Inference}                               &  \checkmark  &    \checkmark  & \checkmark  \\
        \hline

      \end{tabular}
    \caption{Software processing steps. Table is adapted from~\cite{bowring2019exploring}.}
    \label{table:pipeline-steps}
\end{table}


\subsection{Data processing}

To capture variability between tools, the fMRI analysis pipeline was processed using three of the most popular software
packages in neuroimaging, including FSL (version 5.0.10), AFNI (version 18.1.09), and SPM (version SPM12, r7771)
executed with GNU/Octave (version 5.2). The analyses were conducted on the same operating system, CentOS 7.3.
% The computations were performed on a cluster comprised of ...
We ensured that the software versions and all the requisites \tristan{unclear what these are} for running analyses used in all experiments 
were similar to the original study and we encapsulated them in Docker container images.
% We could replicate results of the original study except for AFNI with slightly differences due to the parallelization\dots
% We also used SPM/Octave instead of Matlab to be able to run analysis in HPC and also perturb mathmetical functions
% because Matlab has no dependencies on the GNU mathematical library, and it uses its libraries\dots

In addition, the same subjects and fMRI analysis were processed with the same configurations in the FL environment
to produce the numerical variability results.
We only perturbed the maximum precisions \tristan{``the maximum precisions'' cannot be understood} to compare uncertainty between tool variability and numerical instability at the level of OS.
For this purpose, we applied instrumentations at the virtual precision (t) of 53 bits for double-precision floating-point values
and 24 bits for  single-precision values. Three Fuzzy samples were generated, to match the number of tool samples.

We evaluated variabilities \tristan{define them, introduce BT and WT acronyms} in thresholded and unthresholded group-level activation maps.
We statistically computed \tristan{what's a ``statistical computation''?} voxel-wise standard deviations \tristan{did you check standard error vs standard deviation?} of T-statistic values for each pair of tools
and then compared the correlation of changes \tristan{idk what's a ``correlation of changes''} in both conditions.
The standard deviation between fuzzy samples corresponds to the square root of the summation of variances between samples in each tool \tristan{Rephrase: we computed the WT standard deviation as ...}.
Moreover, we determined region-by-region Dice coefficients for the thresholded maps for each pair of tools \tristan{Explain more. You should add least mention the parcellation used.}.
% This measured the overlap of acitaved voxels which assess the spatial similarity between activated maps.
The Dice values between Fuzzy samples correspond to the average pair-wise Dices computed among three Fuzzy samples
for each pair of tools.
Also, we compared the correlation of Dice scores normalized by the region sizes in both conditions.


\section{Results}
All scripts and results to create the figures in this section are available at \url{https://github.com/ali4006/fuzzy-neurotools}.
Replication of all analyses was visually assessed \tristan{mention what you checked specifically} to be the same as the original study \tristan{cite}.
We ensured that fMRI analyses were processed successfully \tristan{how?}.
In this section, we call the cross-software variation and numerical variation,
between tool (BT) and within tool (WT) variability, respectively \tristan{introduce these notations as early as possible in the methods}. 
The within tool variability shows results among different fuzzy libmath \tristan{capitalize consistently}
samples for the specific version of the tools, and it is not within tool versions \tristan{sentence is cryptic}.

Summary statistics for the group-level (un)thresholded T-statistics are shown in
Table~\ref{table:pipeline-stats}.
The variability between pair of images \tristan{images means MRI} in each condition was measured using the mean and standard deviation of absolute differences \tristan{then call them abs diff in the table}.
Overall, the distribution of the pairwise variations in WT is an order of magnitude lower than BT in both means and standard deviations.
While the highest variability was observed for \fslafni, \fslspm made the least variations for all conditions.
% Overally, we see that values in between tools are comparable to values in within tool results.
\tristan{it's unclear what ``Fuzzy X and Y'' represent.}
\note{Add statistics for subject level (first-level) analyses}

%%%%%%%%%% Summary of statstics %%%%%%%%
\setlength{\tabcolsep}{7pt}
\begin{table}[h]
    \centering
    \begin{tabular}{cccc|cc}
        \toprule
        \multirow{2}{*}{}& {} & \multicolumn{2}{c}{Thresholded} & \multicolumn{2}{c}{Unthresholded} \\
        \cmidrule{3-4} \cmidrule{5-6} \\
        {} & {} & Mean & Std. dev. & Mean & Std. dev. \\
        \midrule
        \rowcolor{lightgray}
        {BT} & FSL vs. SPM          &  0.043       & 1.282      & 0.242     & 0.443  \\
        \rowcolor{lightgray}
        {} & FSL vs. AFNI         &  0.099       & 1.548      & 0.302     & 0.547  \\
        \rowcolor{lightgray}
        {} & AFNI vs. SPM         &  0.079       & 1.475      & 0.254     & 0.608  \\
        {WT} & FSL and SPM    &  0.005       & 0.358      & 0.030     & 0.099  \\
        {} & FSL and AFNI   &  0.011       & 0.475      & 0.038     & 0.155  \\
        {} & AFNI and SPM   &  0.011       & 0.458      & 0.033     & 0.144  \\
        \bottomrule
    \end{tabular}
    \caption{Summary of voxel-wise mean and standard deviation of absolute differences for each pair of tools
    in group-level T-statistics.}
    \label{table:pipeline-stats}
\end{table}


% \subsection{Comparing disparity in BT and WT}
%\subsection{Spatial localization of disparity in BT and WT}
\subsection{Group-level thresholded maps}

%Fig 1
Comparisons of standard deviations between thresholded images in WT and BT
on MNI space are shown in Figure~\ref{fig:thresh-maps}.

While we observe substantial variations in BT with the average standard deviation $\approx$ 1.5,
the order of magnitude of variations in WT is much lower with the average standard deviation $\approx$ 0.5,
as was anticipated. However, numerical variability is still significant in WT results.
%Moreover, the numerical perturbations produces variations in similar order og magnitude in WT compared to BT variations
Moreover, we see the similar order of magnitude in WT variations compared to BT variations
in some regions in the thresholded images.
\note{For instance, some parts of the frontal lobe in the sagittal plane for almost all pairs and the occipital
lobe in the axial plane for the pairs of \fslspm and \fslafni in Figure~\ref{fig:thresh-maps}
are closely replicated using the numerical perturbations.}

% Fig2
In Figure~\ref{fig:dice-thresh}, we compare region-by-region Dice coefficients of
the group-level thresholded maps for all pairs of software packages in BT and WT.
This shows a linear correlation between Dice values in BT and WT,
which implies that the variability of activated voxels in both conditions is similar.
Also, small P-values across all three pairs, including \num{1e-10} for \fslafni, \num{6e-04} for \fslspm,
and \num{2e-05} for \afnispm, confirms this correlation of similarities.
The vertical line where the Dice score is zero in BT shows regions in the brain with no overlaps between activated voxels in BT but WT,
including left/right `V1\_ROI' in \fslafni and \fslspm, and left/right `TGd\_ROI' in \afnispm results.


%%%%%%%%%% Var. of Thresh %%%%%%%%
\begin{figure*}[ht]
    \fbox{\begin{minipage}{\dimexpr \textwidth-2\fboxsep-2\fboxrule}
    \begin{subfigure}[t]{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/bt-wt-thresh-std.pdf}
      %\caption{Standard deviation of thresholded t-statistics map on template surface}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/ratio-thresh-std.pdf}
      %\caption{Standard deviation of thresholded t-statistics map on template surface}
    \end{subfigure}
    \caption{Maps of standard deviation of thresholded T-statistics. The first and second rows show
    maps on BT and WT results, respectively, and the third row represents maps of the ratio between them.}
    % so that bright blue areas indicate more similar order of magnitude of variations in both conditions,
    %and vise versa for the darkder regions.}
    \label{fig:thresh-maps}
    \end{minipage}}
  \end{figure*}
  

  %%%%%%%%%% Dice plot of thresholded tstats%%%%%%%%
  \begin{figure}[b]
    \fbox{\begin{minipage}{\dimexpr \columnwidth-2\fboxsep-2\fboxrule}
    \centering
    \includegraphics[width=\columnwidth]{figures/dices_corr.png}
    \caption{Correlation of Dice coefficients of activated regions in BT and WT
    from the thresholded T-statistics. Different pairs are illustrated in different colors. 
    Regions correspond to the 360 areas of cortical parcellation (HCP-MMP1.0)~\cite{glasser2016multi}.}
    \label{fig:dice-thresh}
    \end{minipage}}
  \end{figure}



\subsection{Group-level unthresholded maps}
% \subsection{Variability of unthresholded statistical maps}

% Fig3
Figure~\ref{fig:unthresh-maps} shows the brain maps of standard deviations for group-level unthresholded T-statistics in WT and BT.
Results show a different magnitude of
the order of variations in WT and BT with standard deviation ranges from $\approx$ 0.12 to $\approx$ 0.5 on average, respectively.
The maps show regions in the brain where the magnitude of standard deviation is close to zero in WT but it exceeds 2.0 in BT,
such as the limbic and frontal lobes in the sagittal plane.
However, there are voxels with a significant correlation between WT and BT that are uniformly distributed across the brain maps.

%%%%%%%%%% Var. of Unthresh %%%%%%%%
\begin{figure*}[b]
    \fbox{\begin{minipage}{\dimexpr \textwidth-2\fboxsep-2\fboxrule}
    \begin{subfigure}[t]{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/bt-wt-unthresh-std.pdf}
      %\caption{Standard deviation of thresholded t-statistics map on template surface}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/ratio-unthresh-std.pdf}
      %\caption{Standard deviation of thresholded t-statistics map on template surface}
    \end{subfigure}
    \caption{Maps of standard deviation of unthresholded T-statistics. The first and second rows show
    maps on BT and WT results, respectively, and the third row represents maps of the ratio between them.}
    \label{fig:unthresh-maps}
    \end{minipage}}
  \end{figure*}
  

% Fig4
The scatter plot in Figure~\ref{fig:unthresh-correlation} represents the correlation of standard deviation in WT and BT variability.
We found two major clusters, including the identity cluster that corresponds to the correlations
between BT and WT with the ratio of $0.5 < BT/WT < 2$ where the standard deviations are bigger than 0.1,
and the upper cluster that shows voxels where BT $\approx$ 0.
The percentage of voxels included in the identity cluster is \%9.9 in \fslspm, \%17.3 in \fslafni, and \%13.8 in \afnispm,
and in the upper cluster is $\approx$ \%1 for each pair of tools.
The identity area is also represented on the MNI space in the second row in Figure~\ref{fig:unthresh-correlation}.
This figure shows the spatial localization of the parts of the brain that BT variability is correlated with the numerical variability.
This refines the presented results in Figure~\ref{fig:unthresh-maps},
which indicates how correlation is uniformly distributed across the brain.
  
  %%%%%%%%%% Corr. plot of tstats%%%%%%%%
  \begin{figure*}[b]
    \fbox{\begin{minipage}{\dimexpr \textwidth-2\fboxsep-2\fboxrule}
      \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/std-corr-unthresh-plot.png}
        %\caption{Standard deviation of thresholded t-statistics map on template surface}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/corr-unthresh-std.pdf}
        %\caption{Standard deviation of thresholded t-statistics map on template surface}
      \end{subfigure}
      \caption{Correlation of standard deviations in BT and WT from the unthresholded T-statistics. 
      The first row plots the correlations voxel by voxel and is highlighted with different colors
      for two clusters, including the upper cluster (purple color) and the correlated cluster (green color).
      The second row maps the correlated area on the MNI space.}
    \label{fig:unthresh-correlation}
    \end{minipage}}
  \end{figure*}
  

  \subsection{Subject-level unthresholded maps}

  Table~\ref{table:firstlevel-stats} shows a summary of the statistics from the subject-level analysis results.
  This table represents the average of the mean and standard deviation of absolute differences of unthresholded T-statistics
  across 16 subjects for each pair of tools.
  Results show that subjects 15 and 10 were produced the least and the most variability, respectively, among all pairs, subjects,
  and both BT and WT.
  While subject 15 generated results with the mean of $\approx$ 0.093 and standard deviation of $\approx$ 0.174,
  subject 10 produced results with the mean of $\approx$ 0.183 and standard deviation of $\approx$ 0.341.
  This observation shows an order of magnitude of variability between subjects.
  % The most variability for \fslspm appeared in subject 1 in BT and subject 11 in WT, and for \fslafni
  % and \afnispm subject 10 in BT and subject 5 in WT were the most variable subjects.
  Moreover, Figure~\ref{fig:unthresh-firstlevel} shows the maps of standard deviations on MNI space in WT and BT for subjects 15 and 10.
  We can see a significant variability between subjects.


%%%%%%%%%% Summary of statstics %%%%%%%%
\setlength{\tabcolsep}{5pt}
\begin{table*}[h]
    \centering
    \begin{tabular}{cccc|cc|cc}
        \toprule
        \multirow{2}{*}{} &{} & \multicolumn{2}{c}{All Subjects} & \multicolumn{2}{c}{Least variable} & \multicolumn{2}{c}{Most variable} \\
        \cmidrule{3-4} \cmidrule{5-6} \cmidrule{7-8} \\
        {} & {} & Mean  & Std. dev.  & Mean & Std. dev. & Mean  & Std. dev. \\
        \midrule
        \rowcolor{lightgray}
        {BT} & FSL vs. SPM           & 0.245     & 0.366       & 0.166     & 0.255    & 0.273    & 0.405  \\
        \rowcolor{lightgray}
        {}   & FSL vs. AFNI          & 0.295     & 0.439      & 0.183     & 0.282     & 0.392    & 0.582  \\
        \rowcolor{lightgray}
        {}   & AFNI vs. SPM          & 0.238     & 0.491      & 0.150     & 0.314     & 0.310    & 0.642  \\
        {WT} & FSL and SPM    & 0.033     & 0.092      & 0.018     & 0.050     & 0.036    & 0.099  \\
        {}   & FSL and AFNI   & 0.040     & 0.136      & 0.021     & 0.075     & 0.048    & 0.165  \\
        {}   & AFNI and SPM   & 0.033     & 0.124      & 0.019     & 0.071     & 0.039    & 0.155  \\
        \bottomrule
    \end{tabular}
    \caption{Summary of voxel-wise mean and standard deviation of absolute differences for each pair of tools
    in subject-level T-statistics.}
    \label{table:firstlevel-stats}
\end{table*}

  
  %%%%%%%%%% Var. of Unthresh %%%%%%%%
  \begin{figure*}[b]
    \fbox{\begin{minipage}{\dimexpr \textwidth-2\fboxsep-2\fboxrule}
      \centering
      \includegraphics[width=\textwidth]{figures/sbj-level-bt-wt-unthresh-std.pdf}
    \caption{Maps of standard deviation of unthresholded T-statistics from subject 15 in the rows A and B,
    and subject 10 in the rows C and D.}
    \label{fig:unthresh-firstlevel}
    \end{minipage}}
  \end{figure*}
  
  
  

\section{Conclusion \& Discussion}

% There are many statistical comparisons, but the neuro-scientific interpretation of results is not on my side.

\begin{itemize}
    \item[$\bullet$ ] In this study, we represented the magnitude of differences in between tool and within tool results.
    We obtained more instability in BT compared to WT. Also we showed how between tool variations
    are correlated with numerical variability.
   
    \item[$\bullet$ ] Generally, we obtained more uncertainty on thresholded results, probably due to different thresholding
    methods used in different tools. This can raise further investigations on the thresholding techniques toward stability.
  
    \item[$\bullet$ ] Further studies can be evaluating the numerical stabilities within tool by focusing on the particular parts of 
    the pipeline that has been identified as the main sources of variations in~\cite{bowring2021isolating}.
    Also, we can invetigate the precision in WT variability that simulates mostly BT variability in the furure study.
    
\end{itemize}
  

\section{Acknowledgments} 


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{plain}
\bibliography{biblio}


\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
